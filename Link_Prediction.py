# DRNL_HDE_4Node_Enhanced.py
# åŸºäºDRNL_HDE_2Loss_update.pyä¿®æ”¹ä¸ºæ”¯æŒ4ä¸ªèŠ‚ç‚¹ç±»å‹çš„å¼‚æ„å›¾
# 4ä¸ªèŠ‚ç‚¹ç±»å‹ï¼šè¢«åŠ¨å…ƒä»¶ã€ä¸»åŠ¨å…ƒä»¶ã€ç”µæºå…ƒä»¶ã€ç½‘ç»œèŠ‚ç‚¹
#è·‘kicad+ltspice+ltspiceæ•°æ®é›†çš„æ—¶å€™ï¼ŒæŠŠdevice_typeæ”¹æˆ=34;æŠŠDATASET=æ”¹æˆå¯¹åº”çš„æ•°æ®é›†
#è·‘spicenetlist+analoggenie+masalachaiæ•°æ®é›†çš„æ—¶å€™ï¼ŒæŠŠdevice_typeæ”¹æˆ=18;æŠŠDATASET=æ”¹æˆå¯¹åº”çš„æ•°æ®é›†
import math
from itertools import chain
import numpy as np
import torch
import torch.nn.functional as F
from scipy.sparse.csgraph import shortest_path
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score, accuracy_score
from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList
from torch_geometric.data import Data, InMemoryDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import MLP, GCNConv, global_sort_pool
from torch_geometric.transforms import RandomLinkSplit
from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix, negative_sampling
from tqdm import tqdm
import GPUtil
import time
import warnings
import matplotlib.pyplot as plt
import os
import networkx as nx
from collections import deque

# Set Device Here
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==================== ä¿®æ”¹çš„å‚æ•°é…ç½® ====================
# Dataset - ã€ä¿®æ”¹1ã€‘æ›´æ”¹ä¸º4èŠ‚ç‚¹å¼‚æ„å›¾æ•°æ®é›†
#DATASET = "SpiceNetlist"
#DATASET = "Masala-CHAI"
DATASET = "KiCad_github"
#DATASET = "LTspice_demos"
#DATASET = "LTspice_examples"
DATASET_ROOT_DIRECTORY = "./data/"
DATASET_PT = DATASET_ROOT_DIRECTORY + DATASET + "_4node_heterogeneous.pt"  # ã€ä¿®æ”¹ã€‘æ•°æ®é›†æ–‡ä»¶å
DATASET_PROCESSED = DATASET + "_4node_heterogeneous_processed.pt"  # ã€ä¿®æ”¹ã€‘å¤„ç†åæ–‡ä»¶å

# HDE Configuration - ã€ä¿®æ”¹2ã€‘æ›´æ–°ä¸º4èŠ‚ç‚¹ç±»å‹
USE_HDE = True
NODE_TYPES = 4  # ã€ä¿®æ”¹ã€‘ä»2æ”¹ä¸º4ï¼šè¢«åŠ¨å…ƒä»¶ + ä¸»åŠ¨å…ƒä»¶ + ç”µæºå…ƒä»¶ + ç½‘ç»œèŠ‚ç‚¹
MAX_DIST = 3
# ã€ä¿®æ”¹3ã€‘æ›´æ–°èŠ‚ç‚¹ç±»å‹æ˜ å°„
HDE_TYPE_MAPPING = {
    'P': 0,  # è¢«åŠ¨å…ƒä»¶ (Passive)
    'A': 1,  # ä¸»åŠ¨å…ƒä»¶ (Active)
    'S': 2,  # ç”µæºå…ƒä»¶ (Source)
    'N': 3  # ç½‘ç»œèŠ‚ç‚¹ (Network)
}


# ==================== å…³é”®æ”¹è¿›ï¼šæ›´ä¼˜çš„è¶…å‚æ•°ï¼ˆä¿æŒä¸å˜ï¼‰====================
N_SPLITS = 5
MIN_NUM_EPOCHS = 8
RANDOM_STATE = 42
MAX_NUM_EPOCHS = 60  # å¢åŠ è®­ç»ƒè½®æ•°
PATIENCE = 6  # å¢åŠ è€å¿ƒå€¼
MIN_IMPROVEMENT = 0.001
LEARNING_RATE = 1e-4  # æé«˜å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-6  # æ·»åŠ æƒé‡è¡°å‡
BATCH_SIZE = 6  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
HIDDEN_CHANNELS = 80  # å¢åŠ éšè—ç»´åº¦
NUM_LAYERS = 4  # å¢åŠ ç½‘ç»œæ·±åº¦
DROPOUT_RATE = 0.5  # æ·»åŠ Dropout
MAX_EPOCHS_WHERE_TEST_ACC_STUCK = 8

# ã€ä¿®æ”¹4ã€‘æ¨¡å‹ä¿å­˜ç›®å½•
MODEL_SAVE_DIRECTORY = "./model-save-4node"
PLOT_SAVE_DIRECTORY = "./plot-4node"
os.makedirs(MODEL_SAVE_DIRECTORY, exist_ok=True)
os.makedirs(PLOT_SAVE_DIRECTORY, exist_ok=True)

warnings.filterwarnings('ignore')


def get_gpu_usage():
    GPUs = GPUtil.getGPUs()
    if len(GPUs) == 0:
        return "No GPU"
    gpu_info = []
    for gpu in GPUs:
        gpu_info.append(f"GPU {gpu.id}: {gpu.load * 100:.1f}% Load, {gpu.memoryUtil * 100:.1f}% Memory")
    return "; ".join(gpu_info)


class MyOwnDataset(InMemoryDataset):
    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):
        super().__init__(root, transform, pre_transform, pre_filter)
        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)

    @property
    def processed_file_names(self):
        return [DATASET_PROCESSED]

    def process(self):
        data_list = [torch.load(DATASET_PT, weights_only=False)]
        if self.pre_filter is not None:
            data_list = [data for data in data_list if self.pre_filter(data)]
        if self.pre_transform is not None:
            data_list = [self.pre_transform(data) for data in data_list]
        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])


def drnl_node_labeling(edge_index, src, dst, num_nodes=None):
    src, dst = (dst, src) if src > dst else (src, dst)
    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()
    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))
    adj_wo_src = adj[idx, :][:, idx]
    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))
    adj_wo_dst = adj[idx, :][:, idx]
    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)
    dist2src = np.insert(dist2src, dst, 0, axis=0)
    dist2src = torch.from_numpy(dist2src)
    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst - 1)
    dist2dst = np.insert(dist2dst, src, 0, axis=0)
    dist2dst = torch.from_numpy(dist2dst)
    dist = dist2src + dist2dst
    dist_over_2, dist_mod_2 = dist // 2, dist % 2
    z = 1 + torch.min(dist2src, dist2dst)
    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)
    z[src] = 1.
    z[dst] = 1.
    z[torch.isnan(z)] = 0.
    return z.to(torch.long)


# ã€ä¿®æ”¹5ã€‘HDEç‰¹å¾æå–å™¨ - æ›´æ–°ä¸º4èŠ‚ç‚¹ç±»å‹
class HDE_Enhanced_Subgraph_Extractor:
    def __init__(self, node_types=4, max_dist=3):  # ã€ä¿®æ”¹ã€‘ä»2æ”¹ä¸º4
        self.node_types = node_types
        self.max_dist = max_dist
        self.type2idx = HDE_TYPE_MAPPING
        self.global_node_types = None

    def prepare_node_types(self, data):
        if hasattr(data, "node_types"):
            self.global_node_types = data.node_types
        else:
            raise ValueError("Data object must have 'node_types' attribute for heterogeneous graph")

    # ã€ä¿®æ”¹6ã€‘æ›´æ–°èŠ‚ç‚¹ç±»å‹æ¨æ–­å‡½æ•°
    def infer_node_type(self, global_node_idx):
        if self.global_node_types is None:
            raise RuntimeError("Call prepare_node_types() first")

        node_type_idx = self.global_node_types[global_node_idx].item()
        # æ˜ å°„åˆ°å¯¹åº”çš„èŠ‚ç‚¹ç±»å‹å­—ç¬¦
        type_map = {0: 'P', 1: 'A', 2: 'S', 3: 'N'}
        return type_map.get(node_type_idx, 'P')  # é»˜è®¤ä¸ºè¢«åŠ¨å…ƒä»¶

    def edge_index_to_networkx(self, edge_index, sub_nodes, data):
        G = nx.Graph()
        for i, node_idx in enumerate(sub_nodes):
            global_idx = node_idx.item()
            node_type = self.infer_node_type(global_idx)
            G.add_node(f"N{i}", type=node_type, original_idx=global_idx)
        for i in range(edge_index.size(1)):
            src, dst = edge_index[:, i].tolist()
            G.add_edge(f"N{src}", f"N{dst}")
        return G

    # ã€ä¿®æ”¹7ã€‘æ›´æ–°HDEè®¡ç®—ä»¥æ”¯æŒ4ç§èŠ‚ç‚¹ç±»å‹
    def compute_node_hde(self, G, node_name, target_name):
        try:
            try:
                shortest_path = nx.shortest_path(G, node_name, target_name)
                if len(shortest_path) - 1 > self.max_dist + 1:
                    return np.zeros(self.node_types * (self.max_dist + 1), dtype=np.float32)
            except nx.NetworkXNoPath:
                return np.zeros(self.node_types * (self.max_dist + 1), dtype=np.float32)

            cnt = [self.max_dist] * self.node_types
            try:
                paths = []
                # ã€ä¿®æ”¹8ã€‘åˆå§‹åŒ–4ç§èŠ‚ç‚¹ç±»å‹è®¡æ•°
                queue = deque([(node_name, [node_name], {'P': 0, 'A': 0, 'S': 0, 'N': 0})])

                while queue:
                    current, path, type_counts = queue.popleft()
                    if current == target_name:
                        paths.append((path, type_counts.copy()))
                        if len(paths) >= 3:
                            break
                        continue
                    if len(path) >= self.max_dist + 2:
                        continue
                    for neighbor in G.neighbors(current):
                        if neighbor not in path:
                            new_type_counts = type_counts.copy()
                            neighbor_type = G.nodes[neighbor].get('type', 'P')
                            if neighbor_type in self.type2idx:
                                new_type_counts[neighbor_type] += 1
                            queue.append((neighbor, path + [neighbor], new_type_counts))

                if not paths:
                    return np.zeros(self.node_types * (self.max_dist + 1), dtype=np.float32)
            except:
                # ã€ä¿®æ”¹9ã€‘å›é€€è·¯å¾„è®¡ç®—æ›´æ–°ä¸º4ç§èŠ‚ç‚¹ç±»å‹
                paths = [(shortest_path,
                          {node_type: shortest_path.count(node_type) - (
                              1 if node_type == G.nodes[node_name].get('type', 'P') else 0)
                           for node_type in ['P', 'A', 'S', 'N']})]

            # ã€ä¿®æ”¹10ã€‘å¤„ç†4ç§èŠ‚ç‚¹ç±»å‹çš„è·¯å¾„
            for path, type_counts in paths:
                res = [0] * self.node_types
                res[0] = type_counts.get('P', 0)  # è¢«åŠ¨å…ƒä»¶
                res[1] = type_counts.get('A', 0)  # ä¸»åŠ¨å…ƒä»¶
                res[2] = type_counts.get('S', 0)  # ç”µæºå…ƒä»¶
                res[3] = type_counts.get('N', 0)  # ç½‘ç»œèŠ‚ç‚¹

                for k in range(self.node_types):
                    if res[k] > 0:
                        cnt[k] = min(cnt[k], res[k])

            one_hot_list = []
            for i in range(self.node_types):
                count_val = min(cnt[i], self.max_dist)
                one_hot = np.eye(self.max_dist + 1, dtype=np.float32)[count_val]
                one_hot_list.append(one_hot)

            return np.concatenate(one_hot_list)
        except Exception as e:
            print(f"HDE computation error: {e}")
            return np.zeros(self.node_types * (self.max_dist + 1), dtype=np.float32)

    def compute_subgraph_hde(self, sub_nodes, sub_edge_index, src, dst, data):
        try:
            G = self.edge_index_to_networkx(sub_edge_index, sub_nodes, data)
            node_mapping = {i: f"N{i}" for i in range(len(sub_nodes))}
            hde_matrix = []
            for i, node_idx in enumerate(sub_nodes):
                node_name = node_mapping[i]
                src_name = node_mapping[src]
                dst_name = node_mapping[dst]
                if node_name in G.nodes and src_name in G.nodes and dst_name in G.nodes:
                    dist_to_src = self.compute_node_hde(G, node_name, src_name)
                    dist_to_dst = self.compute_node_hde(G, node_name, dst_name)
                    hde_feature = np.concatenate([dist_to_src, dist_to_dst])
                else:
                    hde_feature = np.zeros(self.node_types * (self.max_dist + 1) * 2, dtype=np.float32)
                hde_matrix.append(hde_feature)
            return torch.FloatTensor(np.array(hde_matrix))
        except Exception as e:
            print(f"HDE computation failed: {e}")
            zero_feature = np.zeros((len(sub_nodes), self.node_types * (self.max_dist + 1) * 2), dtype=np.float32)
            return torch.FloatTensor(zero_feature)


def extract_enclosing_subgraphs(edge_index, edge_label_index, y, num_hops, data, global_max_z=None, use_hde=False):
    data_list = []
    local_max_z = 0
    hde_extractor = None
    if use_hde:
        hde_extractor = HDE_Enhanced_Subgraph_Extractor(node_types=NODE_TYPES, max_dist=MAX_DIST)
        hde_extractor.prepare_node_types(data)

    for src, dst in edge_label_index.t().tolist():
        sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(
            [src, dst], num_hops, edge_index, relabel_nodes=True, num_nodes=data.x.size(0))
        src, dst = mapping.tolist()
        mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)
        mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)
        sub_edge_index = sub_edge_index[:, mask1 & mask2]
        z = drnl_node_labeling(sub_edge_index, src, dst, num_nodes=sub_nodes.size(0))
        local_max_z = max(local_max_z, int(z.max()))
        node_features = data.x[sub_nodes]

        if use_hde and hde_extractor:
            hde_features = hde_extractor.compute_subgraph_hde(sub_nodes, sub_edge_index, src, dst, data)
            if hde_features is not None and hde_features.size(0) == node_features.size(0):
                node_features = torch.cat([node_features, hde_features], dim=1)
            else:
                zero_hde = torch.zeros((node_features.size(0), NODE_TYPES * (MAX_DIST + 1) * 2),
                                       dtype=node_features.dtype)
                node_features = torch.cat([node_features, zero_hde], dim=1)

        data_item = Data(x=node_features, z=z, edge_index=sub_edge_index, y=y)
        data_list.append(data_item)

    max_z = global_max_z if global_max_z is not None else local_max_z
    for data_item in data_list:
        data_item.z = torch.clamp(data_item.z, max=max_z)
        one_hot = F.one_hot(data_item.z, max_z + 1).to(torch.float)
        data_item.x = torch.cat([one_hot, data_item.x], dim=1)

    return data_list


# ==================== ã€ä¿®æ”¹11ã€‘å¼ºåŒ–çš„æŸå¤±å‡½æ•° - æ›´æ–°ä¸º4èŠ‚ç‚¹ç±»å‹ ====================
class EnhancedHeterogeneousLoss(torch.nn.Module):
    """
    ä¸“ä¸º4èŠ‚ç‚¹å¼‚æ„å›¾è®¾è®¡çš„å¼ºåŒ–æŸå¤±å‡½æ•°
    å……åˆ†åˆ©ç”¨18ç§è®¾å¤‡ç±»å‹å’Œ4ç§èŠ‚ç‚¹ç±»å‹ï¼ˆè¢«åŠ¨ã€ä¸»åŠ¨ã€ç”µæºã€ç½‘ç»œï¼‰å¼‚æ„ç‰¹æ€§
    """

    def __init__(self, alpha=0.4, beta=0.35, gamma=0.2, delta=0.05,
                 node_types=4, device_types=34, use_hde=True):  # ã€ä¿®æ”¹ã€‘node_typesä»2æ”¹ä¸º4
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.delta = delta
        self.node_types = node_types  # ã€ä¿®æ”¹ã€‘ç°åœ¨æ˜¯4ä¸ªèŠ‚ç‚¹ç±»å‹
        self.device_types = device_types
        self.use_hde = use_hde

        # åŸºç¡€æŸå¤±
        self.bce_loss = BCEWithLogitsLoss(reduction='none')

        # å¯å­¦ä¹ çš„è®¾å¤‡ç±»å‹é‡è¦æ€§æƒé‡
        self.device_weights = torch.nn.Parameter(torch.ones(device_types) / device_types)

    def forward(self, predictions, targets, batch_data=None):
        device = predictions.device

        # 1. è‡ªé€‚åº”BCEæŸå¤±
        base_loss = self._compute_adaptive_bce_loss(predictions, targets)

        # 2. 4èŠ‚ç‚¹è®¾å¤‡ç±»å‹æ„ŸçŸ¥æŸå¤± - æ ¸å¿ƒæ”¹è¿›
        device_loss = torch.tensor(0.0, device=device)
        if batch_data is not None and self.use_hde:
            try:
                device_loss = self._compute_4node_device_type_loss(predictions, targets, batch_data)
            except Exception as e:
                print(f"4-node device type loss error (skipping): {e}")
                device_loss = torch.tensor(0.0, device=device)

        # 3. 4èŠ‚ç‚¹æ‹“æ‰‘ç»“æ„æŸå¤±
        topology_loss = torch.tensor(0.0, device=device)
        if batch_data is not None and self.use_hde:
            try:
                topology_loss = self._compute_4node_topology_loss(predictions, targets, batch_data)
            except Exception as e:
                print(f"4-node topology loss error (skipping): {e}")
                topology_loss = torch.tensor(0.0, device=device)

        # 4. ç®€åŒ–çš„å¯¹æ¯”æŸå¤±
        contrast_loss = torch.tensor(0.0, device=device)
        try:
            contrast_loss = self._compute_simple_contrast_loss(predictions, targets)
        except Exception as e:
            print(f"Contrast loss error (skipping): {e}")
            contrast_loss = torch.tensor(0.0, device=device)

        # ç»„åˆæŸå¤±
        total_loss = (self.alpha * base_loss +
                      self.beta * device_loss +
                      self.gamma * topology_loss +
                      self.delta * contrast_loss)

        return total_loss

    def _compute_adaptive_bce_loss(self, predictions, targets):
        """è‡ªé€‚åº”BCEæŸå¤±"""
        bce = self.bce_loss(predictions.view(-1), targets.float())
        probs = torch.sigmoid(predictions.view(-1))
        uncertainty = 1.0 - torch.abs(probs - 0.5) * 2
        adaptive_weights = 1.0 + 2.0 * uncertainty
        return (bce * adaptive_weights).mean()

    # ã€ä¿®æ”¹12ã€‘æ–°å¢4èŠ‚ç‚¹è®¾å¤‡ç±»å‹æ„ŸçŸ¥æŸå¤±
    def _compute_4node_device_type_loss(self, predictions, targets, batch_data):
        """4èŠ‚ç‚¹è®¾å¤‡ç±»å‹æ„ŸçŸ¥æŸå¤±"""
        if not hasattr(batch_data, 'x') or batch_data.x.size(1) < 34:
            return torch.tensor(0.0, device=predictions.device)

        device_features = batch_data.x[:, :34]
        batch_indices = batch_data.batch
        unique_batches = torch.unique(batch_indices)

        device_loss = 0.0
        valid_count = 0

        for batch_idx in unique_batches:
            try:
                mask = batch_indices == batch_idx
                if mask.sum() == 0:
                    continue

                subgraph_devices = device_features[mask]
                device_dist = subgraph_devices.sum(dim=0) + 1e-8
                device_dist = device_dist / device_dist.sum()

                if batch_idx.item() >= len(predictions):
                    continue

                pred_prob = torch.sigmoid(predictions[batch_idx.item()])
                target_val = targets[batch_idx.item()].float()

                device_importance = torch.sum(self.device_weights * device_dist)
                weighted_error = torch.abs(pred_prob - target_val) * device_importance

                # ã€ä¿®æ”¹13ã€‘4èŠ‚ç‚¹ç±»å‹å¤šæ ·æ€§å¥–åŠ±
                # å¯¹äº4ç§èŠ‚ç‚¹ç±»å‹ï¼Œå¢åŠ ç±»å‹å¤šæ ·æ€§å¥–åŠ±
                entropy = -torch.sum(device_dist * torch.log(device_dist + 1e-8))
                # 4èŠ‚ç‚¹ç±»å‹çš„å¤šæ ·æ€§æƒ©ç½šæ›´åŠ å¤æ‚
                diversity_penalty = entropy * torch.abs(pred_prob - target_val) * 0.15  # å¢åŠ æƒé‡

                device_loss += weighted_error + diversity_penalty
                valid_count += 1

            except Exception as e:
                continue

        return device_loss / max(valid_count, 1)

    # ã€ä¿®æ”¹14ã€‘4èŠ‚ç‚¹æ‹“æ‰‘ä¸€è‡´æ€§æŸå¤±
    def _compute_4node_topology_loss(self, predictions, targets, batch_data):
        """4èŠ‚ç‚¹æ‹“æ‰‘ä¸€è‡´æ€§æŸå¤±"""
        if not hasattr(batch_data, 'x'):
            return torch.tensor(0.0, device=predictions.device)

        batch_indices = batch_data.batch
        unique_batches = torch.unique(batch_indices)
        topology_loss = 0.0
        valid_count = 0

        for batch_idx in unique_batches:
            try:
                mask = batch_indices == batch_idx
                if mask.sum() < 2:
                    continue

                subgraph_features = batch_data.x[mask]

                if self.use_hde and subgraph_features.size(1) > 19:
                    feature_dim = subgraph_features.size(1)

                    if feature_dim >= 32:
                        # ã€ä¿®æ”¹15ã€‘å‡è®¾4èŠ‚ç‚¹HDEç‰¹å¾åœ¨å32ç»´ (4 * (3+1) * 2 = 32)
                        hde_start = feature_dim - 32
                        drnl_part = subgraph_features[:, :hde_start]
                        hde_part = subgraph_features[:, hde_start:]

                        if drnl_part.size(1) > 0 and hde_part.size(1) > 0:
                            drnl_summary = drnl_part.mean(dim=0)
                            hde_summary = hde_part.mean(dim=0)

                            if drnl_summary.size(0) != hde_summary.size(0):
                                min_dim = min(drnl_summary.size(0), hde_summary.size(0))
                                drnl_summary = drnl_summary[:min_dim]
                                hde_summary = hde_summary[:min_dim]

                            if batch_idx.item() >= len(predictions):
                                continue

                            pred_conf = torch.abs(torch.sigmoid(predictions[batch_idx.item()]) - 0.5) * 2

                            if drnl_summary.numel() > 0 and hde_summary.numel() > 0:
                                feature_sim = F.cosine_similarity(
                                    drnl_summary.unsqueeze(0),
                                    hde_summary.unsqueeze(0)
                                ).abs()

                                consistency = torch.abs(feature_sim - pred_conf)
                                # ã€ä¿®æ”¹16ã€‘4èŠ‚ç‚¹çš„ä¸€è‡´æ€§æŸå¤±æƒé‡è°ƒæ•´
                                topology_loss += consistency * 1.2  # å¢åŠ 4èŠ‚ç‚¹ä¸€è‡´æ€§é‡è¦æ€§
                                valid_count += 1

            except Exception as e:
                continue

        return topology_loss / max(valid_count, 1)

    def _compute_simple_contrast_loss(self, predictions, targets):
        """ç®€åŒ–çš„å¯¹æ¯”æŸå¤±"""
        probs = torch.sigmoid(predictions.view(-1))

        pos_mask = targets == 1
        neg_mask = targets == 0

        if pos_mask.sum() == 0 or neg_mask.sum() == 0:
            return torch.tensor(0.0, device=predictions.device)

        pos_probs = probs[pos_mask]
        neg_probs = probs[neg_mask]

        # ç®€å•çš„è¾¹ç•ŒæŸå¤±ï¼šæ­£æ ·æœ¬åº”è¯¥>0.6ï¼Œè´Ÿæ ·æœ¬åº”è¯¥<0.4
        pos_loss = F.relu(0.6 - pos_probs).mean()
        neg_loss = F.relu(neg_probs - 0.4).mean()

        return pos_loss + neg_loss


# ==================== å¼ºåŒ–çš„æ¨¡å‹æ¶æ„ï¼ˆä¿æŒEnhancedHDE_DGCNNä¸å˜ï¼‰====================
class EnhancedHDE_DGCNN(torch.nn.Module):
    """å®Œå…¨ä¿®å¤ç‰ˆæœ¬ - è§£å†³æ‰€æœ‰ç»´åº¦ä¸åŒ¹é…é—®é¢˜"""

    def __init__(self, hidden_channels, num_layers, num_features=None, k=0.6,
                 node_types=4, max_dist=3, use_hde=True, dropout=0.25):  # ã€ä¿®æ”¹ã€‘node_typesä»2æ”¹ä¸º4
        super().__init__()
        if num_features is None:
            raise ValueError("num_features must be specified")

        self.use_hde = use_hde
        if k < 1:
            self.k = 15
        else:
            self.k = int(k)

        print(f"é˜²å¼¹æ¨¡å‹åˆå§‹åŒ–: k={self.k}, features={num_features}, hidden={hidden_channels}")
        print(f"4èŠ‚ç‚¹ç±»å‹: è¢«åŠ¨å…ƒä»¶ã€ä¸»åŠ¨å…ƒä»¶ã€ç”µæºå…ƒä»¶ã€ç½‘ç»œèŠ‚ç‚¹")

        # å›¾å·ç§¯å±‚ - ç®€å•ç¨³å®šçš„è®¾è®¡
        self.convs = ModuleList()
        self.convs.append(GCNConv(num_features, hidden_channels))
        for i in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
        self.convs.append(GCNConv(hidden_channels, 1))  # ä¿æŒåŸè®¾è®¡

        self.dropout = torch.nn.Dropout(dropout)

        # å…³é”®æ”¹è¿›ï¼šå»¶è¿Ÿåˆ›å»ºåˆ†ç±»å™¨ï¼Œæ ¹æ®å®é™…è¾“å‡ºåŠ¨æ€è°ƒæ•´
        self.classifier = None
        self.hidden_channels = hidden_channels
        self.num_layers = num_layers
        self.dropout_rate = dropout

        print(f"é˜²å¼¹æ¨¡å‹: å»¶è¿Ÿåˆå§‹åŒ–åˆ†ç±»å™¨")

    def _create_classifier(self, input_dim):
        """æ ¹æ®å®é™…è¾“å…¥ç»´åº¦åˆ›å»ºåˆ†ç±»å™¨"""
        print(f"åŠ¨æ€åˆ›å»ºåˆ†ç±»å™¨: input_dim={input_dim}")

        if input_dim >= 512:
            classifier = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 256),
                torch.nn.BatchNorm1d(256),
                torch.nn.ReLU(),
                torch.nn.Dropout(self.dropout_rate),
                torch.nn.Linear(256, 128),
                torch.nn.ReLU(),
                torch.nn.Dropout(self.dropout_rate * 0.5),
                torch.nn.Linear(128, 64),
                torch.nn.ReLU(),
                torch.nn.Linear(64, 1)
            )
        elif input_dim >= 128:
            classifier = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 128),
                torch.nn.ReLU(),
                torch.nn.Dropout(self.dropout_rate),
                torch.nn.Linear(128, 64),
                torch.nn.ReLU(),
                torch.nn.Linear(64, 1)
            )
        elif input_dim >= 64:
            classifier = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 64),
                torch.nn.ReLU(),
                torch.nn.Dropout(self.dropout_rate),
                torch.nn.Linear(64, 32),
                torch.nn.ReLU(),
                torch.nn.Linear(32, 1)
            )
        else:
            classifier = torch.nn.Sequential(
                torch.nn.Linear(input_dim, max(16, input_dim // 2)),
                torch.nn.ReLU(),
                torch.nn.Linear(max(16, input_dim // 2), 1)
            )

        return classifier

    def forward(self, x, edge_index, batch):
        # å›¾å·ç§¯éƒ¨åˆ†
        xs = [x]
        for conv in self.convs:
            xs += [conv(xs[-1], edge_index).tanh()]

        # ç‰¹å¾èåˆ
        x = torch.cat(xs[1:], dim=-1)  # è·³è¿‡è¾“å…¥ç‰¹å¾

        # å…¨å±€æ’åºæ± åŒ–
        x = global_sort_pool(x, batch, self.k)

        # å…³é”®ï¼šåŠ¨æ€åˆ›å»ºå’Œä½¿ç”¨åˆ†ç±»å™¨
        actual_dim = x.size(1)

        if self.classifier is None:
            self.classifier = self._create_classifier(actual_dim).to(x.device)
        elif self.classifier[0].in_features != actual_dim:
            # å¦‚æœç»´åº¦å˜äº†ï¼Œé‡æ–°åˆ›å»º
            print(f"ç»´åº¦å˜åŒ–: {self.classifier[0].in_features} -> {actual_dim}")
            self.classifier = self._create_classifier(actual_dim).to(x.device)

        return self.classifier(x)


# ==================== å¼ºåŒ–çš„è®­ç»ƒå‡½æ•°ï¼ˆæ›´æ–°ä¸º4èŠ‚ç‚¹ï¼‰====================
def enhanced_train(model, loader, optimizer, scheduler, criterion):
    model.train()
    total_loss = 0
    y_pred, y_true = [], []

    # ã€ä¿®æ”¹17ã€‘ä½¿ç”¨4èŠ‚ç‚¹å¼ºåŒ–æŸå¤±å‡½æ•°
    if USE_HDE:
        enhanced_criterion = EnhancedHeterogeneousLoss(
            alpha=0.4,  # åŸºç¡€BCE
            beta=0.35,  # 4èŠ‚ç‚¹è®¾å¤‡ç±»å‹æ„ŸçŸ¥ï¼ˆé‡ç‚¹ï¼‰
            gamma=0.2,  # 4èŠ‚ç‚¹æ‹“æ‰‘ä¸€è‡´æ€§
            delta=0.05,  # éš¾æ ·æœ¬æŒ–æ˜
            node_types=NODE_TYPES,  # ã€ä¿®æ”¹ã€‘4èŠ‚ç‚¹ç±»å‹
            device_types=34,
            use_hde=USE_HDE
        ).to(device)
        print("Using 4-node enhanced heterogeneous dual encoding loss")
    else:
        enhanced_criterion = criterion

    with tqdm(loader, desc="Training", unit="batch", mininterval=10) as tepoch:
        for batch_idx, data in enumerate(tepoch):
            data = data.to(device)
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            out = model(data.x, data.edge_index, data.batch)

            # æŸå¤±è®¡ç®—
            if USE_HDE:
                loss = enhanced_criterion(out.view(-1), data.y, batch_data=data)
            else:
                loss = criterion(out.view(-1), data.y.to(torch.float))

            # åå‘ä¼ æ’­ + æ¢¯åº¦è£å‰ª
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # å­¦ä¹ ç‡è°ƒåº¦
            if scheduler is not None:
                scheduler.step()

            total_loss += float(loss) * data.num_graphs
            y_pred.append(out.view(-1).cpu().detach())
            y_true.append(data.y.view(-1).cpu().to(torch.float))

            # æ›´æ–°è¿›åº¦æ¡
            tepoch.set_postfix({
                'loss': f'{float(loss):.4f}',
                'lr': f'{optimizer.param_groups[0]["lr"]:.6f}',
                'gpu': get_gpu_usage().split(':')[1].split('%')[0] + '%' if 'GPU' in get_gpu_usage() else 'CPU'
            })

    train_loss = total_loss / len(loader.dataset)
    y_pred = torch.cat(y_pred)
    y_true = torch.cat(y_true)
    train_auc = roc_auc_score(y_true, y_pred)
    y_pred_binary = (torch.sigmoid(y_pred) >= 0.5).int()
    train_acc = accuracy_score(y_true, y_pred_binary)

    return train_loss, train_auc, train_acc


@torch.no_grad()
def test(model, loader, mode="Validation"):
    model.eval()
    y_pred, y_true = [], []
    with tqdm(loader, desc=mode, unit="batch", mininterval=10) as tepoch:
        for data in tepoch:
            data = data.to(device)
            logits = model(data.x, data.edge_index, data.batch)
            y_pred.append(logits.view(-1).cpu())
            y_true.append(data.y.view(-1).cpu().to(torch.float))
    y_pred = torch.cat(y_pred)
    y_true = torch.cat(y_true)
    roc_auc = roc_auc_score(y_true, y_pred)
    y_pred_binary = (torch.sigmoid(y_pred) >= 0.5).int()
    accuracy = accuracy_score(y_true, y_pred_binary)
    return roc_auc, accuracy


def compute_global_max_z(data, train_edges, test_edges):
    global_max_z = 0
    transform = RandomLinkSplit(num_val=0.1, num_test=0.0, is_undirected=True,
                                split_labels=True, add_negative_train_samples=True)
    train_data, val_data, _ = transform(Data(edge_index=train_edges, x=data.x))
    test_neg_edge_index = negative_sampling(edge_index=train_edges, num_nodes=data.x.size(0),
                                            num_neg_samples=test_edges.size(1), method="sparse")

    all_edge_pairs = [
        (train_data.edge_index, train_data.pos_edge_label_index),
        (train_data.edge_index, train_data.neg_edge_label_index),
        (val_data.edge_index, val_data.pos_edge_label_index),
        (val_data.edge_index, val_data.neg_edge_label_index),
        (train_edges, test_edges),
        (train_edges, test_neg_edge_index)
    ]

    for edge_index, edge_label_index in all_edge_pairs:
        for src, dst in edge_label_index.t().tolist():
            try:
                sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(
                    [src, dst], 2, edge_index, relabel_nodes=True, num_nodes=data.x.size(0))
                src, dst = mapping.tolist()
                mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)
                mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)
                sub_edge_index = sub_edge_index[:, mask1 & mask2]
                z = drnl_node_labeling(sub_edge_index, src, dst, num_nodes=sub_nodes.size(0))
                global_max_z = max(global_max_z, int(z.max()))
            except Exception as e:
                print(f"Warning: Error computing z for edge ({src}, {dst}): {e}")
                continue
    return global_max_z


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


# ==================== ã€ä¿®æ”¹18ã€‘å®Œæ•´çš„4èŠ‚ç‚¹KæŠ˜å®éªŒå‡½æ•° ====================
def run_enhanced_kfold_experiment(n_splits=5, num_epochs=60):
    """è¿è¡Œ4èŠ‚ç‚¹å¢å¼ºç‰ˆKæŠ˜äº¤å‰éªŒè¯å®éªŒ"""
    print("=" * 60)
    print("å¯åŠ¨4èŠ‚ç‚¹å¢å¼ºç‰ˆå¼‚æ„å›¾é“¾è·¯é¢„æµ‹å®éªŒ")
    print(f"4èŠ‚ç‚¹ç±»å‹ï¼šè¢«åŠ¨å…ƒä»¶ã€ä¸»åŠ¨å…ƒä»¶ã€ç”µæºå…ƒä»¶ã€ç½‘ç»œèŠ‚ç‚¹")
    print(f"ç›®æ ‡ï¼šValidation AUC > 0.92, Test AUC > 0.90, Test Acc > 0.85")
    print("=" * 60)

    dataset = MyOwnDataset(DATASET_ROOT_DIRECTORY)
    data = dataset[0]

    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    edge_indices = data.edge_index.t().cpu().numpy()

    val_auc_scores = []
    test_auc_scores = []
    test_acc_scores = []

    for fold, (train_idx, test_idx) in enumerate(kf.split(edge_indices)):
        restart_fold = True
        max_restarts = 5
        restart_count = 0

        while restart_fold and restart_count < max_restarts:
            restart_fold = False
            if restart_count > 0:
                print(f"\né‡å¯ç¬¬ {fold + 1}/{n_splits} æŠ˜ (ç¬¬ {restart_count + 1} æ¬¡å°è¯•)")
            else:
                print(f"\nç¬¬ {fold + 1}/{n_splits} æŠ˜å®éªŒå¼€å§‹")

            train_edges = data.edge_index[:, train_idx]
            test_edges = data.edge_index[:, test_idx]

            # è®¡ç®—global_max_z
            global_max_z = compute_global_max_z(data, train_edges, test_edges)
            print(f"Global max z: {global_max_z}")

            # æ•°æ®åˆ†å‰²
            transform = RandomLinkSplit(num_val=0.1, num_test=0.0, is_undirected=True,
                                        split_labels=True, add_negative_train_samples=True)
            train_data, val_data, _ = transform(Data(edge_index=train_edges, x=data.x))

            # å­å›¾æå–
            print("æ­£åœ¨æå–å°é—­å­å›¾ï¼ˆ4èŠ‚ç‚¹HDEå¢å¼ºï¼‰...")
            train_pos_data = extract_enclosing_subgraphs(
                train_data.edge_index, train_data.pos_edge_label_index, 1, 2, data, global_max_z, use_hde=USE_HDE)
            train_neg_data = extract_enclosing_subgraphs(
                train_data.edge_index, train_data.neg_edge_label_index, 0, 2, data, global_max_z, use_hde=USE_HDE)
            val_pos_data = extract_enclosing_subgraphs(
                val_data.edge_index, val_data.pos_edge_label_index, 1, 2, data, global_max_z, use_hde=USE_HDE)
            val_neg_data = extract_enclosing_subgraphs(
                val_data.edge_index, val_data.neg_edge_label_index, 0, 2, data, global_max_z, use_hde=USE_HDE)
            test_pos_data = extract_enclosing_subgraphs(train_edges, test_edges, 1, 2, data, global_max_z,
                                                        use_hde=USE_HDE)
            neg_edge_index = negative_sampling(edge_index=train_edges, num_nodes=data.x.size(0),
                                               num_neg_samples=test_edges.size(1), method="sparse")
            test_neg_data = extract_enclosing_subgraphs(train_edges, neg_edge_index, 0, 2, data, global_max_z,
                                                        use_hde=USE_HDE)

            # æ•°æ®åŠ è½½å™¨
            train_dataset = train_pos_data + train_neg_data
            val_dataset = val_pos_data + val_neg_data
            test_dataset = test_pos_data + test_neg_data

            effective_batch_size = min(BATCH_SIZE, max(1, len(train_dataset) // 20))
            train_loader = DataLoader(train_dataset, batch_size=effective_batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=effective_batch_size)
            test_loader = DataLoader(test_dataset, batch_size=effective_batch_size)

            # æ¨¡å‹å‚æ•°
            num_nodes_list = sorted([d.num_nodes for d in train_dataset])
            k = num_nodes_list[int(math.ceil(0.6 * len(num_nodes_list))) - 1]
            k = max(34, k)  # å¢åŠ kå€¼
            num_features = train_dataset[0].x.size(1)

            print(f"4èŠ‚ç‚¹æ¨¡å‹é…ç½®: features={num_features}, k={k}, hidden={HIDDEN_CHANNELS}")

            # ã€ä¿®æ”¹19ã€‘åˆ›å»º4èŠ‚ç‚¹å¢å¼ºæ¨¡å‹
            model = EnhancedHDE_DGCNN(
                hidden_channels=HIDDEN_CHANNELS,
                num_layers=NUM_LAYERS,
                num_features=num_features,
                k=k,
                node_types=NODE_TYPES,  # ã€ä¿®æ”¹ã€‘4èŠ‚ç‚¹ç±»å‹
                max_dist=MAX_DIST,
                use_hde=USE_HDE,
                dropout=DROPOUT_RATE
            ).to(device)

            # æ¨¡å‹å‚æ•°åˆå§‹åŒ–
            for name, param in model.named_parameters():
                if 'weight' in name and param.dim() > 1:
                    if 'conv' in name:
                        torch.nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')
                    else:
                        torch.nn.init.xavier_normal_(param)
                elif 'bias' in name:
                    torch.nn.init.constant_(param, 0)

            # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=LEARNING_RATE,
                weight_decay=WEIGHT_DECAY,
                betas=(0.9, 0.999),
                eps=1e-8
            )

            total_steps = len(train_loader) * num_epochs
            warmup_steps = total_steps // 10
            scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)

            criterion = BCEWithLogitsLoss()

            # è®­ç»ƒè®°å½•
            train_losses, train_aucs, train_accs = [], [], []
            val_aucs, val_accs = [], []
            test_aucs, test_accs = [], []

            best_val_auc = 0
            best_test_auc = 0
            best_test_acc = 0

            early_stop_best_val_acc = -float('inf')
            patience_counter = 0

            print(f"å¼€å§‹4èŠ‚ç‚¹è®­ç»ƒ (ç›®æ ‡: {num_epochs} epochs)")

            # è®­ç»ƒå¾ªç¯
            for epoch in range(1, num_epochs + 1):
                # è®­ç»ƒ
                train_loss, train_auc, train_acc = enhanced_train(
                    model, train_loader, optimizer, scheduler, criterion
                )

                # éªŒè¯å’Œæµ‹è¯•
                val_auc, val_acc = test(model, val_loader, "Validation")
                test_auc, test_acc = test(model, test_loader, "Testing")

                # æ—©åœæ£€æŸ¥
                if epoch == MAX_EPOCHS_WHERE_TEST_ACC_STUCK and abs(test_acc - 0.5) < 1e-4:
                    print(f"æµ‹è¯•å‡†ç¡®ç‡åœæ»åœ¨0.5ï¼Œé‡å¯ç¬¬ {fold + 1} æŠ˜...")
                    restart_fold = True
                    restart_count += 1
                    break

                # è®°å½•æŒ‡æ ‡
                train_losses.append(train_loss)
                train_aucs.append(train_auc)
                train_accs.append(train_acc)
                val_aucs.append(val_auc)
                val_accs.append(val_acc)
                test_aucs.append(test_auc)
                test_accs.append(test_acc)

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                    best_test_auc = test_auc
                    best_test_acc = test_acc
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'global_max_z': global_max_z,
                        'k': k,
                        'num_features': num_features,
                        'use_hde': USE_HDE,
                        'node_types': NODE_TYPES,  # ã€ä¿®æ”¹ã€‘ä¿å­˜4èŠ‚ç‚¹ç±»å‹ä¿¡æ¯
                        'best_val_auc': best_val_auc,
                        'best_test_auc': best_test_auc,
                        'best_test_acc': best_test_acc
                    }, f"{MODEL_SAVE_DIRECTORY}/4node_enhanced_model_fold{fold + 1}.pth")

                # æ—©åœæœºåˆ¶
                if val_acc > early_stop_best_val_acc + MIN_IMPROVEMENT:
                    early_stop_best_val_acc = val_acc
                    patience_counter = 0
                else:
                    if epoch > MIN_NUM_EPOCHS:
                        patience_counter += 1
                    if patience_counter > PATIENCE:
                        print(f"ç¬¬ {epoch} è½®è§¦å‘æ—©åœ (ç¬¬ {fold + 1} æŠ˜)")
                        break

                # ã€ä¿®æ”¹20ã€‘æ‰“å°è¿›åº¦ - 4èŠ‚ç‚¹çŠ¶æ€
                status = "4-Node-Enhanced" if USE_HDE else "Original"
                progress_bar = "â–ˆ" * int(val_auc * 10) + "â–‘" * (10 - int(val_auc * 10))
                print(f"[{status}] ç¬¬{fold + 1}æŠ˜ Epoch {epoch:02d} | "
                      f"Train: L={train_loss:.4f} AUC={train_auc:.4f} Acc={train_acc:.4f} | "
                      f"Val: AUC={val_auc:.4f} Acc={val_acc:.4f} | "
                      f"Test: AUC={test_auc:.4f} Acc={test_acc:.4f} | "
                      f"{progress_bar}")

                # è¾¾åˆ°ç›®æ ‡æ£€æŸ¥
            #   if val_auc >= 0.92 and test_auc >= 0.90 and test_acc >= 0.85:
            #       print(f"è¾¾åˆ°ç›®æ ‡æŒ‡æ ‡ï¼æå‰å®Œæˆç¬¬ {fold + 1} æŠ˜")
            #       break

            if restart_fold:
                continue

            # å¯è§†åŒ–ç»“æœï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
            plt.figure(figsize=(15, 5))

            plt.subplot(1, 3, 1)
            plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title(f'4-Node Training Loss - Fold {fold + 1}')
            plt.legend()
            plt.grid(True)

            plt.subplot(1, 3, 2)
            plt.plot(range(1, len(train_aucs) + 1), train_aucs, 'g-', label='Train AUC')
            plt.plot(range(1, len(val_aucs) + 1), val_aucs, 'r-', label='Val AUC')
            plt.plot(range(1, len(test_aucs) + 1), test_aucs, 'b-', label='Test AUC')
            plt.axhline(y=0.90, color='orange', linestyle='--', label='Target AUC')
            plt.xlabel('Epoch')
            plt.ylabel('AUC Score')
            plt.title(f'4-Node AUC Scores - Fold {fold + 1}')
            plt.legend()
            plt.grid(True)

            plt.subplot(1, 3, 3)
            plt.plot(range(1, len(train_accs) + 1), train_accs, 'g-', label='Train Acc')
            plt.plot(range(1, len(val_accs) + 1), val_accs, 'r-', label='Val Acc')
            plt.plot(range(1, len(test_accs) + 1), test_accs, 'b-', label='Test Acc')
            plt.axhline(y=0.85, color='orange', linestyle='--', label='Target Acc')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.title(f'4-Node Accuracy - Fold {fold + 1}')
            plt.legend()
            plt.grid(True)

            plt.tight_layout()
            plt.savefig(f'{PLOT_SAVE_DIRECTORY}/4node_enhanced_results_fold{fold + 1}.png', dpi=300)
            plt.close()

            # è®°å½•æœ€ä½³ç»“æœ
            val_auc_scores.append(best_val_auc)
            test_auc_scores.append(best_test_auc)
            test_acc_scores.append(best_test_acc)

            print(
                f"ç¬¬ {fold + 1} æŠ˜å®Œæˆ | æœ€ä½³4èŠ‚ç‚¹ç»“æœ: Val AUC={best_val_auc:.4f}, Test AUC={best_test_auc:.4f}, Test Acc={best_test_acc:.4f}")

        if restart_count >= max_restarts:
            print(f"ç¬¬ {fold + 1} æŠ˜åœ¨ {max_restarts} æ¬¡é‡è¯•åä»æœªæ”¶æ•›ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            val_auc_scores.append(0.5)
            test_auc_scores.append(0.5)
            test_acc_scores.append(0.5)

    # æœ€ç»ˆç»“æœæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("FINAL 4-NODE ENHANCED RESULTS")
    print("=" * 80)

    mean_val_auc = np.mean(val_auc_scores)
    std_val_auc = np.std(val_auc_scores)
    mean_test_auc = np.mean(test_auc_scores)
    std_test_auc = np.std(test_auc_scores)
    mean_test_acc = np.mean(test_acc_scores)
    std_test_acc = np.std(test_acc_scores)

    print(f"Average Validation AUC: {mean_val_auc:.4f} Â± {std_val_auc:.4f}")
    print(f"Average Test AUC:       {mean_test_auc:.4f} Â± {std_test_auc:.4f}")
    print(f"Average Test Accuracy:  {mean_test_acc:.4f} Â± {std_test_acc:.4f}")

    # ç›®æ ‡è¾¾æˆæ£€æŸ¥
    targets_met = []
    if mean_val_auc >= 0.92:
        targets_met.append("âœ… Validation AUC â‰¥ 0.92")
    else:
        targets_met.append(f"âŒ Validation AUC < 0.92 (å·®è·: {0.92 - mean_val_auc:.4f})")

    if mean_test_auc >= 0.90:
        targets_met.append("âœ… Test AUC â‰¥ 0.90")
    else:
        targets_met.append(f"âŒ Test AUC < 0.90 (å·®è·: {0.90 - mean_test_auc:.4f})")

    if mean_test_acc >= 0.85:
        targets_met.append("âœ… Test Accuracy â‰¥ 0.85")
    else:
        targets_met.append(f"âŒ Test Accuracy < 0.85 (å·®è·: {0.85 - mean_test_acc:.4f})")

    print("\nç›®æ ‡è¾¾æˆæƒ…å†µ:")
    for target in targets_met:
        print(f"   {target}")

    print(f"\nå„æŠ˜è¯¦ç»†ç»“æœ:")
    for i, (val_auc, test_auc, test_acc) in enumerate(zip(val_auc_scores, test_auc_scores, test_acc_scores)):
        status = "â­" if val_auc >= 0.92 and test_auc >= 0.90 and test_acc >= 0.85 else "ğŸ“Š"
        print(f"   ç¬¬{i + 1}æŠ˜ {status}: Val AUC={val_auc:.4f}, Test AUC={test_auc:.4f}, Test Acc={test_acc:.4f}")

    print("=" * 80)

    return val_auc_scores, test_auc_scores, test_acc_scores


# ==================== ã€ä¿®æ”¹21ã€‘ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    print("å¯åŠ¨4èŠ‚ç‚¹å¢å¼ºç‰ˆDRNL-HDEå¼‚æ„å›¾é“¾è·¯é¢„æµ‹å®éªŒ")
    print(f"å®éªŒé…ç½®:")
    print(f"   - ä½¿ç”¨4èŠ‚ç‚¹HDEå¢å¼º: {USE_HDE}")
    print(f"   - èŠ‚ç‚¹ç±»å‹: {NODE_TYPES} (è¢«åŠ¨å…ƒä»¶ + ä¸»åŠ¨å…ƒä»¶ + ç”µæºå…ƒä»¶ + ç½‘ç»œèŠ‚ç‚¹)")
    print(f"   - spicenetlist/analoggenie/masalachaiè®¾å¤‡ç±»å‹: 18ç§")
    print(f"   - kicad_github/ltspice_demosè®¾å¤‡ç±»å‹: 34ç§")
    print(f"   - å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"   - éšè—ç»´åº¦: {HIDDEN_CHANNELS}")
    print(f"   - ç½‘ç»œå±‚æ•°: {NUM_LAYERS}")
    print(f"   - Dropout: {DROPOUT_RATE}")
    print(f"   - æœ€å¤§è½®æ•°: {MAX_NUM_EPOCHS}")
    print(f"ç›®æ ‡: Val AUC â‰¥ 0.92, Test AUC â‰¥ 0.90, Test Acc â‰¥ 0.85")
    print("=" * 80)
    print("4èŠ‚ç‚¹å¼‚æ„å›¾ä¿®æ”¹æ€»ç»“:")
    print("1. æ›´æ–°NODE_TYPESä»2æ”¹ä¸º4")
    print("2. æ›´æ–°HDE_TYPE_MAPPINGåŒ…å«P, A, S, NèŠ‚ç‚¹ç±»å‹")
    print("3. ä¿®æ”¹HDE_Enhanced_Subgraph_Extractoræ”¯æŒ4èŠ‚ç‚¹ç±»å‹")
    print("4. å¢å¼ºEnhancedHeterogeneousLossçš„4èŠ‚ç‚¹å¹³è¡¡")
    print("5. æ›´æ–°æ¨¡å‹æ¶æ„å¤„ç†4èŠ‚ç‚¹HDEç‰¹å¾")
    print("6. ä¿®æ”¹æ•°æ®é›†æ–‡ä»¶è·¯å¾„ä½¿ç”¨4node_heterogeneousæ•°æ®")
    print("7. æ·»åŠ 4èŠ‚ç‚¹ç±»å‹å¹³è¡¡æŸå¤±è®¡ç®—")
    print("8. å¢å¼º4èŠ‚ç‚¹åŒé‡ç¼–ç ä¸€è‡´æ€§æŸå¤±")
    print("9. æ›´æ–°æ‰€æœ‰æ—¥å¿—å’ŒçŠ¶æ€æ¶ˆæ¯æ”¯æŒ4èŠ‚ç‚¹")
    print("=" * 80)
    print("-" * 60)

    # è¿è¡Œå®éªŒ
    val_scores, test_auc_scores, test_acc_scores = run_enhanced_kfold_experiment(N_SPLITS, MAX_NUM_EPOCHS)

    print("\nå®éªŒå®Œæˆï¼4èŠ‚ç‚¹ç»“æœå·²ä¿å­˜åˆ° ./model-save-4node/ å’Œ ./plot-4node/ ç›®å½•")